---
title: 'STAT665 Final Project: TITLE'
author: "Rose Brewin, Emily Dodwell, Samantha Emanuele"
date: "May 5, 2015"
output: 
  pdf_document:
    toc: true    
    toc_depth: 3
    number_sections: true
    fig_caption: true
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Load required packages
library(knitr)
library(kernlab)
```

\newpage

# Introduction

In this paper, we explore various machine learning classification algorithms to determine the most effective and computationally efficient method by which to identify the language in which a sentence has been written (as either English or other), by recognition of letter frequency.  Such a classification mechanism has many relevant applications, including web services that identify when text must be translated to the user's default language.  An advantage for using machine learning methods for such a task is the lack of reliance on a language dictionary; that is, the algorithm does not depend on prior knowledge of the given language, including definitions, grammar structures, and correct use of accents and punctuation.

Given this lack of reliance on a single "dictionary", our choice of a best method may then be extended to consider how different contexts may use language differently.  For example, \textit{The New York Times} has an extensive language styleguide that may differentiate the formality of its articles from more colloquial interactions, such as social media posts on Twitter or Instagram.

```{r, echo = FALSE}
# Read in single letter files
x <- read.csv("trainSingle.csv", stringsAsFactors = FALSE)
y <- read.csv("testSingle.csv", stringsAsFactors = FALSE)
  
# Recode language (0 = English)
x$language <- ifelse(x$language == 0, 0, 1)
y$language <- ifelse(y$language == 0, 0, 1)
```

# Methodology for Data Collection

We first consider observed frequency of twenty-seven characters; specifically the twenty-six letters of the Roman alphabet and a blank space.  

## Data Collection

To train the various classfication algorithms considered, we first found plain text documents of the Bible in various languages (citation needed HERE): English, French, Spanish, Latin, German, Dutch, and Italian.  Our decision to use the Bible was based on its status as a universal text, and it therefore acts as a control among the languages considered.  The accessibility of plain text Bibles written in these languages informed their choosing, and because the goal of our model is to identify English sentences from non-English sentences, we were interested in a mixture of Romance languages that are Latin derivatives.  For each of these text documents, we cleaned them in the following manner:

> * Upon scanning the text document into R, save each new line of text as an individual observation in a list.
> * Convert all letters to lowercase, split each observation into a vector of individual characters, and remove any character (including any accented letter) that is not a space or letter in the Roman alphabet.
> * Create a 1200 $\times$ 28 data frame of training data, where the empirical distribution of the 27 characters (26 letters and a space) in each observation is a row.  The 28th column is a language indicator, where 0 represents that the observation was originally written in English and 1 denotes another language.  The first 600 rows are randomly chosen lines from the English language Bible, and the remaining rows consist of 100 randomly chosen lines from each of the other Bibles.

We demonstrate our methodology with a simple example:

```{r, results='asis'}
example <- "Machine Learning is ~*SuPeR FuN*~ and we learned so much :) 
We hope you enjoy our project!"

myletters <- c(letters, " ")

clean <- function(string){
  vec <- tolower(string)
  vec <- unlist(strsplit(vec, split=""))
  vec <- vec[vec %in% myletters]
  return(vec)
}

cleaned <- clean(example); cleaned

features <- function(vec) {
  vec <- as.numeric(sapply(myletters, FUN = function(x){round(mean(vec==x),2)}))
  vec[is.na(vec)] <- 0
  dat <- as.data.frame(t(vec))
  names(dat) <- myletters
  return(dat)
}

kable(features(cleaned)[,1:15])
kable(features(cleaned)[,16:27])
```

## Models Considered

For our stated purpose, we consider four classification algorithms:

* Logistic Regression
* Support Vector Machine with three different kernels:
    * Linear
    * Gaussian
    * Kullback-Leibler

To assess which of these models is the "best" at correctly classifying Bible sentences as written in English or another language, we evaluate the predictive ability of each model by its misclassification rate when applied to a test set of 400 randomly-selected English Bible sentences and 400 randomly-selected sentences from the other Bibles (constructed in a manner similar to that described for the training data).  We also make note of the amount of time required to train each model as a measure of its computational efficiency.

# Logistic Regression

We first consider a logistic regression model as a baseline against which to compare other machine learning algorithms due to its simplicity and accessibility as a classification device for a binary categorical reponse variable.  Because the fitted values of a logitistic regression model are in the form of probabilities, this model also enables us to identify those lines of test data that the model incorrectly classifies or otherwise considers somewhat ambiguous.

```{r, warning=FALSE, echo=FALSE, fig.cap="Predictions of Logistic Regression Model"}
# Model and prediction
  then <- Sys.time()
lr.0 <- glm(language ~ ., data = x, family = "binomial")
  now <- Sys.time()
pr.0 <- predict(lr.0, newdata = y, type = "response")

# Make predictions binary (0 = English, 1 = other)
pr.b <- ifelse(pr.0 < .5, 0, 1)

# Misclassification rate
mis.l <- round(mean(pr.b != y$language)*100, 2)

# Computational time
time.l <- round(as.numeric(now-then), 2)

# Plot
y$error <- 2 # Incorrect Classification
y$error[pr.b == y$language] <- 3 # Correct Classification
par(mar=c(5.1, 5.1, 5.1, 8.1), xpd=TRUE)
plot(pr.0, ylab="Prediction",xlab = 'Observation', main="Logistic Regression",
     col = y$error, pch = 16)
legend('right', inset = c(-0.3,0),c('Correct', 'Incorrect'), col = c(3,2), pch = 16)
invisible(dev.off()) # Reset Graph Options
```

As evident by the logistic regression model's misclassification rate of `r mis.l`% on the test set, this provides a very good prediction of a sentence's language.

Figure 1 shows the preidctions for each observation from the logisitic regresison model.  The red points represent any observation that was misclassified while the green points represents any observation that was correctly classified. This figure reveals how the logisitic regression model was able to correctly classify a majority of the observations. The first half of the data is English observations which is why we are seeing observations 1-300 closer to 0 and 301-600 closer to 1.

Keep in mind when studying the plot of predicted probabilities above that test set is structured such that the first 400 observations are English.  The graph demonstrates that the model is unsure of how to classify observations that consist of a single word (which may be the same in other languages).  They 23 misclassified observations correspond to the following lines from the English Bible:

```{r, echo=FALSE}
# Finding which lines of the bible were misclassified. Note that this code only 
# works for english lines that were misclassified. More work needed if we want
# other languages.
misclass <- which(pr.b != y$language) # Find which are misclassified
misclass <- misclass[misclass <= 300] # Only want the english misclassifications

dictionary <- read.csv("testIndex.csv") 
misclass <- dictionary$original[misclass] # Find which line of english bible had this index
raw <- read.csv("englishRaw.csv", stringsAsFactors=F)
misclass <- raw$x[misclass] # Actual bible lines
dat <- data.frame("Misclassified Lines"=misclass)
print(dat)
```

# Support Vector Machine

## Linear Kernel

```{r}
# Model and prediction
then <- Sys.time()
sv.l <- ksvm(language ~ ., data=x, kernel="vanilladot", scaled=F, type="C-svc")
now <- Sys.time()
pr.svl <- predict(sv.l, newdata = y, type = "response")

# Misclassification rate
mis.svl <- round(mean(pr.svl != y$language)*100, 2)

# Computational time
time.svl <- round(as.numeric(now-then), 2)
```

```{r, echo = FALSE, fig.cap = "Predicted Outcomes from Linear Kernelâ€}
# Create error term
y$error <- 2 # Incorrect Classification
y$error[pr.svl == y$language] <- 3 # Correct Classification

# Plot
par(mar=c(5.1, 5.1, 5.1, 8.1), xpd=TRUE)
plot(jitter(pr.svl), ylab="Prediction",xlab = 'Observation', main="Linear Kernel",
     col = y$error, pch = 16)
legend('right', inset = c(-0.3,0),c('Correct', 'Incorrect'), col = c(3,2), pch = 16)
invisible(dev.off()) # Reset Graph Options
```

Figure 2 shows the predicted response probabilities for each of the 600 observations using a linear kernel SVM. We begin to see more red observations compared to the logistic regression model, suggesting that more observations are being misclassified.

## Gaussian Kernel

```{r, echo = FALSE, warning= FALSE, message = FALSE}
# Model and prediction
then <- Sys.time()
sv.g <- ksvm(language ~ ., data=x, kernel="rbfdot", scaled=F, type="C-svc")
now <- Sys.time()
pr.svg <- predict(sv.g, newdata = y, type = "response")

# Misclassification rate
mis.svg <- round(mean(pr.svg != y$language)*100, 2)

# Computational time
time.svg <- round(as.numeric(then-now), 2)
```

```{r, echo = FALSE, fig.cap = "Predicted Outcomes from Gaussian Kernel"}
# Create error term
y$error <- 2 # Incorrect Classification
y$error[pr.svg == y$language] <- 3 # Correct Classification

# Plot
par(mar=c(5.1, 5.1, 5.1, 8.1), xpd=TRUE)
plot(jitter(pr.svg), ylab="Prediction",xlab = 'Observation', main="Gaussian Kernel",
     col = y$error, pch = 16)
legend('right', inset = c(-0.3,0),c('Correct', 'Incorrect'), col = c(3,2), pch = 16)
invisible(dev.off()) # Reset Graph Options
```

The predicted values from the Gaussian Kernel SVM are shown in Figure 3.  This figure appears to have less red observations compared to the linear kernel, but still not as few as the logistic regression model.  This suggests that the Gaussian kernel may be better at detecting the English language compared to the linear kernel, but still not as well as the logistic regression model.

## Kullback-Leibler Kernel

The feature vectors in our model are of the form of relative frequencies. We can take advantage of this structure by treating them as empirical probability distributions. We assume that languages have an underlying distribution on the frequency of letters, and that from this distribution various sentences have been randomly created. 

To determine which underlying distribution a given sentence comes from, we can use techniques from statistical theory. We will use a kernel based on the Kullback-Leibler Divergence between two distributions

$$D(p \|  q) = \sum_{i=1}^n p_i \log\left(\frac{p_i}{q_i}\right) $$

In order to use this as a kernel in a support vector machine, we must ensure it is symmetric and positive semidefinite. **CITIATION** suggests the following transformation as a kernel:
$$k(x, y) = \exp(-a(D(x\|y) + D(y\|x)))$$
where $a$ is a parameter to be chosen.

Using the \texttt{kernlab} package, we implemented this SVM on our language classification data set. We immediately noticed that this runs far slower than any of the alternative algorithms, which we suspect is to some extent because the package is not optimised for our kernel.

We experimented with different values of the parameter $a$, and found that a value of **SOMETHING** minimised the misclassification rate on the test set. With this we were able to achieve a misclassification rate of **SOMETHING**.

Because this is a user-defined fuction, we present the code below for clarity:

```{r, warning=FALSE, cache=TRUE}
# Add very small value to all values so 0 is not an issue
xnew <- x; ynew <- y
xnew[,-1] <- xnew[,-1] + .000000000000000001
ynew[,-1] <- ynew[,-1] + .000000000000000001

KL <- function(p, q) {
  vec <- p*log(p/q)
  return(sum(vec, na.rm=T))
}

mykernel <- function(p, q) {
  exp(-0.01*(KL(p, q) + KL(q, p)))  
}

class(mykernel) <- "kernel"

# Model and prediction
  then <- Sys.time()
sv.k <- ksvm(language ~ ., data=xnew, kernel=mykernel, type="C-svc")
  now <- Sys.time()
pr.svk <- predict(sv.k, newdata = ynew, type = "response")

# Misclassification rate
mis.svk <- round(mean(pr.svk != y$language)*100, 2) #0.242381

# Computational time
time.svk <- round(as.numeric(then-now), 2)
```

```{r, echo = FALSE, fig.cap = "Predicted Outcomes from Kullback-Leibler Kernel"}
# Create error term
# y$error <- 2 # Incorrect Classification
# y$error[pr.svk == y$language] <- 3 # Correct Classification

# Plot
# par(mar=c(5.1, 5.1, 5.1, 8.1), xpd=TRUE)
# plot(jitter(pr.svk), ylab="Prediction",xlab = 'Observation', main="Kullback-Leibler Kernel",
#      col = y$error, pch = 16)
# legend('right', inset = c(-0.3,0),c('Correct', 'Incorrect'), col = c(3,2), pch = 16)
# invisible(dev.off()) # Reset Graph Options
```

# Discussion of Models and Choice of "Best"

We present in the table below a summary of each model's misclassification rate on the test set, as well as the amount of time elapsed during each model's training.

```{r, results='asis', echo=FALSE}
# Table of model, time elapsed, misclassification error?
models <- c("Logistic", "Linear SVM", "Gaussian SVM", "Kullback-Leibler SVM")
classerrors <- c(mis.l, mis.svl, mis.svg, mis.svk)
times <- c(time.l, time.svl, time.svg, time.svk)
errortable <- data.frame(Model=models, Misclassification=classerrors, Time=times)

kable(errortable)

# Include example of sentence being classified?
```

The logistic regression model appears to perform better that the support vector machine algorithms in both misclassification rate and computational efficiency.

# Two-letter Combinations

We saw that our logistic regression model performed well when trained on the empirical distributions of single letter frequencies, and we wondered if we could reduce the misclassification rate even further by considering the empirical distributions of pairs of adjacent characters as they appeared in our original observations.  For example, the letter pair **"EXAMPLE"** is frequent in German, but appears seldom in English.  We hypothesized that the greater specificity of a data set with $27 \choose 2$ columns may enable the creation of a more powerful model.

We thus trained our previous two top-performing models, logistic regression and SVM with Gaussian kernel, on a new data set.  As noted in the table below, this time the SVM was a clear winner, and using pairs of letters reduced the misclassification rate to SOMETHING.  As noted in STAT665 Lecture 15 on STAT665, support vector machines perform best when there is a great deal of information available, and we suspect that the increase of information available to the model enabled it to surpass the predictive ability of the logistic regression model.

```{r, echo=FALSE}
## Read in two-letter files
a <- read.csv("trainDouble.csv", stringsAsFactors = FALSE)
b <- read.csv("testDouble.csv", stringsAsFactors = FALSE)
  
# Recode language (0 = English)
a$language <- ifelse(a$language == 0, 0, 1)
b$language <- ifelse(b$language == 0, 0, 1)

## Logistic regression model and prediction
lr.d <- glm(language ~ ., data = a, family = "binomial")
prlr.d <- predict(lr.d, newdata = b, type = "response")

  # Make predictions binary (0 = English, 1 = other)
  prlr.db <- ifelse(prlr.d < .5, 0, 1)
  
  # Misclassification rate
  round(mean(prlr.db != b$language)*100, 2)

## SVM Gaussian model and prediction
svg.d <- ksvm(language ~ ., data=a, kernel="rbfdot", scaled=F, type="C-svc")
prsvg.d <- predict(svg.d, newdata = b, type = "response")

  # Misclassification rate
  round(mean(prsvg.d != b$language)*100, 2)
```

```{r, echo = FALSE, fig.cap = "Predicted Outcomes from Logistic Regression and Gaussian Kernel (Double Letter Combination)"}
# Create error term for logistic
b$error <- 2 # Incorrect Classification
b$error[prlr.db == b$language] <- 3 # Correct Classification

# Create error term for Gaussian
b$error.g <- 2 # Incorrect Classification
b$error.g[prsvg.d == b$language] <- 3 # Correct Classification

# Plot
par(mfrow = c(1,2))
plot(jitter(prlr.d), ylab="Prediction",xlab = 'Observation', main="Logistic Regression",
    col = b$error, pch = 16)
plot(jitter(prsvg.d), ylab="Prediction",xlab = 'Observation', main="Gaussian Kernel",
     col = b$error.g, pch = 16)
invisible(dev.off()) # Reset Graph Options
```

When looking at the combination of double letters, we see the Gaussian kernel performs much better than the logistic regresison model.  This is evident from the two plots in Figure 5.  In these plots we see there is much more misclassification for the logistic regression model compared to the Gaussian kernel.

# Twitter/NYTimes Application

We have demonstrated that a support vector machine with (gaussian kernel) has great success at differentiating sentences written in the English language from sentences written in other languages, even though they may be closely related to English.  We are also interested in the extension of this finding to different uses of the English language.  Formal writing differs dramatically in style, punctuation, sentence structure, and vocabulary from more casual interactions, such as those that take place through social media platforms that make frequent use of abbreviations and slang.  The simplicity of our model meant that it did not consider, and we are curious as to whether or not it is still effective when challenged to distinguish between formal and colloquial use of the English language.

All editors and writers for \textit{The New York Times} are expected to uphold the regulations set forth in \textit{The New York Times Manual of Style and Usage}; for this reason, we train our model with sentences pulled from several articles in this newspaper.  (Note: because our cleaning function separates text into sentences along the period "." it separates words such as "Ms." and "Mr." from the remainder of the sentence to which they may belong.  This is a weakness of which we are aware.)  We train our model on colloquial usage of the English language with public twitter data.

## Collection of NYTimes and Twitter Data (Sam)

Using our "best" method, we tried to correctly classify between social media messages from Twitter and text from \textit{The New York Times} articles.  In order to do this, we created a dataset similar to that of the Bible consisting of 1000 observations from Twitter and 1000 observations from \textit{The New York Times}.  For the Twitter data, we extracted 1000 of the most recent tweets using their API. Each tweet was then cleaned and the emperical distribution of letters was obtained using the methodology mentioned above.  This entire process was repeated for \textit{The New York Times} data, where we scraped the top stories using their API.

```{r, warning = FALSE, message = FALSE}
# Twitter is language == 1
app <- read.csv("tweets_nytimes.csv", stringsAsFactors = FALSE)

# Create test and training sets
set.seed(1)
these <- sample(1001:2000, 700, replace = FALSE)
train <- app[c(1:700, these),]
test <- app[-as.numeric(rownames(train)),]

## Logistic regression model and prediction
lr.app <- glm(language ~ ., data = train, family = "binomial")
prlr.app <- predict(lr.app, newdata = test, type = "response")

  # Make predictions binary (0 = English, 1 = other)
  prlr.appb <- ifelse(prlr.app < .5, 0, 1)

  plot(prlr.appb)

  # Misclassification rate
  round(mean(prlr.appb != test$language)*100, 2)

## SVM with Gaussian kernel
svg.app <- ksvm(language ~ ., data=train, kernel="rbfdot", scaled=F, type="C-svc")

  # Identify chosen sigma
  svg.app@kernelf@kpar$sigma
  pr.svgapp <- predict(svg.app, newdata = test, type = "response")

  # Misclassification rate
  1-mean(pr.svgapp == test$language)
```

```{r, echo = FALSE, fig.cap = "Predicted Outcomes from Logistic Regression and Gaussian Kernel (NY Times Application)"}
# Create error term for logistic
test$error <- 2 # Incorrect Classification
test$error[prlr.appb == test$language] <- 3 # Correct Classification

test$error.g <- 2 # Incorrect Classification for Gaussian
test$error.g[pr.svgapp == test$language] <- 3 # Correct Classification

# Plot
par(mfrow = c(1,2))
plot(jitter(prlr.appb), ylab="Prediction",xlab = 'Observation', main="Logistic Regression",
     col = test$error, pch = 16)
plot(jitter(pr.svgapp), ylab="Prediction",xlab = 'Observation', main="Gaussian Kernel",
     col = test$error.g, pch = 16)
invisible(dev.off()) # Reset Graph Options
```

Figure 6 compares the predicted values from the two models, logistic regression and guassian kernel SVM.  The red observations represent misclassification of Twitter data compared to NY Times data. The green represents a correct classification. The Gaussian kernel clearly has less red observations, implying that the Gaussian performs better when trying to determine between the Twitter and NY Times data.

## Findings of model once applied to this task

Subleties: Capitalization, punctuation (differences in use)

## Conclusion